{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3639e010",
   "metadata": {},
   "source": [
    "# NLP Pipelines\n",
    "\n",
    "This package is designed to generalize the fit/predict to entire end to end pipelines for text. It can be used as individual methods or as a pipeline object to manage execution of training and/or testing. See README.md for an overview of which methods are in here.\n",
    "\n",
    "## This Demo Set\n",
    "\n",
    "This set of demos walks through:\n",
    "\n",
    "* **Basic Usage Demos** Using abstract/toy datasets to show how to use this library.\n",
    "  * **Usage of the library as individual methods**: Using each of the methods manually to get results.\n",
    "  * **Usage of the pipeline constructor**: A reimplementaiton of the examples above using the pipeline object.\n",
    "  * **Saving and loading trained pipelines**: Quick guidance on how to separate training and running/prediction.\n",
    "* **Practical Demos**: With real data and justifying decisions on pipeline design.\n",
    "  * **Practical Demo: Clustering**: A real end-to-end example on clustering, where \"clustering\" is picking some set of groups without explicit mapping to classes.\n",
    "  * **Practical Demo: Classification**: A real end-to-end example on classification, where \"classificaiton\" is picking which of a set of classes best apply to a document.\n",
    "  * **Practical Demo: Extractive Labeling**: A real end-to-end example on extractive labeling, where \"extractive labeling\" is picking which words or phrases in a document \"seem important\", like automatic keyword extraction.\n",
    "  * **Practical Demo: Extractive Labeling**: A real end-to-end example on predictive labeling, where a set of possible labels is chosen a prioro, and between none and all of the possible labels may apply to a document.\n",
    "  * **(WIP) Practical Demo: Mixing Methods**:  A real end-to-end example on using different kinds of methods together to get simpler results.\n",
    "\n",
    "NOTE that this is not intended to be an exhaustive demo of all of the different methods: `test_demos.ipynb` may be a better choice for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ad3c6",
   "metadata": {},
   "source": [
    "## Setup Dependncies\n",
    "For portability, this is included to install dependencies for this library and notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc01a498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (3.9.1)\n",
      "Requirement already satisfied: spacy in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (3.8.7)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.2.6)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (2.3.1)\n",
      "Requirement already satisfied: fasttext in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.9.3)\n",
      "Requirement already satisfied: sentence_transformers in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (5.0.0)\n",
      "Requirement already satisfied: rank_bm25 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (0.2.2)\n",
      "Requirement already satisfied: keybert in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.9.0)\n",
      "Requirement already satisfied: multi_rake in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.0.2)\n",
      "Requirement already satisfied: yake in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (0.6.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (2.7.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (3.5)\n",
      "Requirement already satisfied: xgboost in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (3.0.2)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (4.54.1)\n",
      "Requirement already satisfied: hdbscan in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (0.8.40)\n",
      "Requirement already satisfied: umap in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (0.1.1)\n",
      "Requirement already satisfied: pyarrow in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (21.0.0)\n",
      "Requirement already satisfied: bertopic in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (0.17.3)\n",
      "Requirement already satisfied: umap-learn in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (0.5.9.post2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 2)) (2025.7.29)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 5)) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from fasttext->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 7)) (0.34.3)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 7)) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 7)) (4.14.1)\n",
      "Requirement already satisfied: rich>=10.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from keybert->-r requirements.txt (line 11)) (14.0.0)\n",
      "Requirement already satisfied: pycld2>=0.41 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from multi_rake->-r requirements.txt (line 12)) (0.42)\n",
      "Requirement already satisfied: pyrsistent>=0.14.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from multi_rake->-r requirements.txt (line 12)) (0.20.0)\n",
      "Requirement already satisfied: jellyfish in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from yake->-r requirements.txt (line 13)) (1.2.0)\n",
      "Requirement already satisfied: segtok in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from yake->-r requirements.txt (line 13)) (1.5.11)\n",
      "Requirement already satisfied: tabulate in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from yake->-r requirements.txt (line 13)) (0.9.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from torch->-r requirements.txt (line 14)) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from torch->-r requirements.txt (line 14)) (1.14.0)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from torch->-r requirements.txt (line 14)) (2025.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (0.5.3)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from bertopic->-r requirements.txt (line 21)) (6.2.0)\n",
      "Requirement already satisfied: llvmlite>0.36.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from bertopic->-r requirements.txt (line 21)) (0.44.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from umap-learn->-r requirements.txt (line 22)) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from umap-learn->-r requirements.txt (line 22)) (0.5.13)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers->-r requirements.txt (line 7)) (1.1.5)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from plotly>=4.7.0->bertopic->-r requirements.txt (line 21)) (2.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (2025.7.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from rich>=10.4.0->keybert->-r requirements.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from rich>=10.4.0->keybert->-r requirements.txt (line 11)) (2.19.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 14)) (1.3.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 3)) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 3)) (1.5.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 3)) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 3)) (7.3.0.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from jinja2->spacy->-r requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert->-r requirements.txt (line 11)) (0.1.2)\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 3)) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "%pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6366d56",
   "metadata": {},
   "source": [
    "# Basic Usage Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71427222",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We should make different datasets for different pipelines. For this basic usage section, the datasets are fake and chosen unrealistically to demonstrate the shape of the inputs and outputs of the steps and pipelines. We'll need a dataset with no truths (true labels are called \"truths\" in this library) for clustering and extractive labeling (n.b. this is not strictly necessary, clustering and extractive labeling methods will simply ignore truths, so including them here is likely to be confusing), one with classes for classification, and one with lists of labels as truths for labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e188325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_pipelines.dataset import Dataset\n",
    "\n",
    "# methods which do not need truths of any form get the unlabeled_dataset.\n",
    "texts_1 = [\n",
    "    \"I love hiking in the mountains.\",\n",
    "    \"The sun is shining bright today.\",\n",
    "    \"Reading books is my favorite hobby.\",\n",
    "    \"I enjoy outdoor activities like camping.\",\n",
    "    \"It's a beautiful day for a walk in the park.\",\n",
    "    \"I prefer staying indoors and watching movies.\",\n",
    "    \"Cooking new recipes is always fun.\"\n",
    "]\n",
    "\n",
    "unlabeled_dataset = Dataset(texts_1)\n",
    "\n",
    "# methods which need truth clases (exactly one per document) get the classed_dataset.\n",
    "texts_2 = [\n",
    "    \"I love this movie\",\n",
    "    \"This is terrible\",\n",
    "    \"Fantastic work\",\n",
    "    \"Awful experience\",\n",
    "    \"My favorite film ever\",\n",
    "    \"A waste of my time\",\n",
    "    \"Not enough people have seen this masterpiece\"\n",
    "    ]\n",
    "classes = [\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"positive\"\n",
    "    ]\n",
    "classed_dataset = Dataset(texts_2, classes)\n",
    "\n",
    "# methods which need truth label sets (0 to n per document) get the labeled_dataset.\n",
    "texts_3 = [\n",
    "    \"I love baking with things from my garden.\",\n",
    "    \"Here's a recipe for lasagna.\",\n",
    "    \"I enjoy hiking in the mountains and camping.\",\n",
    "    \"I have recenly learned how to cook good food when hiking.\",\n",
    "    \"I spent the weekend building a treehouse.\",\n",
    "    \"Cooking dinner with fresh ingredients is a joy.\",\n",
    "    \"I often take long walks in the park during summer to get inspired on what I should plant.\"\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    [\"cooking\", \"gardening\"],\n",
    "    [\"cooking\"],\n",
    "    [\"outdoors\"],\n",
    "    [\"outdoors\", \"cooking\"],\n",
    "    [\"outdoors\"],\n",
    "    [\"cooking\"],\n",
    "    [\"outdoors\", \"gardening\"]\n",
    "]\n",
    "\n",
    "labeled_dataset = Dataset(texts_3, labels)\n",
    "\n",
    "#also, for convenience, let's add an explicit list of possible_labels\n",
    "possible_labels = [\"gardening\", \"cooking\", \"outdoors\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f8f8c",
   "metadata": {},
   "source": [
    "## Usage of the library as individual methods\n",
    "\n",
    "Now, let's try to get some results for our toy data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8311467",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "As a reminder, we want to apply clusters on the unlabeled dataset.\n",
    "For now, we'll judge the results on what *feels* right for such a small dataset with no truth for comparison; proper evaluation will be explored in the next section on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc54522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial data: <Dataset with 7 texts\n",
      "Texts: ['I love hiking in the mountains.', 'The sun is shining bright today.']... +5 more>\n",
      "after stopword remove: <Dataset with 7 texts\n",
      "Texts: ['love hiking mountains', 'sun shining bright today']... +5 more>\n",
      "after stemming: <Dataset with 7 texts\n",
      "Texts: ['love hike mountain', 'sun shine bright today']... +5 more>\n",
      "after vectorization: <Dataset with 7 texts, vectors: 384-dim\n",
      "Texts: ['love hike mountain', 'sun shine bright today']... +5 more>\n",
      "after k-means: <Dataset with 7 texts, vectors: 384-dim, results: 7 items\n",
      "Texts: ['love hike mountain', 'sun shine bright today']... +5 more\n",
      "Results: [1 1]... +5 more>\n",
      "original text: ['I love hiking in the mountains.', 'The sun is shining bright today.', 'Reading books is my favorite hobby.', 'I enjoy outdoor activities like camping.', \"It's a beautiful day for a walk in the park.\", 'I prefer staying indoors and watching movies.', 'Cooking new recipes is always fun.']\n",
      "cluster results: [1 1 1 0 1 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# let's make a copy of our data as not to interfere with any other runs\n",
    "import copy\n",
    "\n",
    "cluster_dataset = copy.deepcopy(unlabeled_dataset)\n",
    "\n",
    "#Our goal is to see which clusters the data is in, rather than to use the clusters to classify new data, so we can use the same data to fit and predict without splitting, just using cluster_dataset\n",
    "\n",
    "# we need a vectorizer to put this into a form which we can do math on. So we can use one thing for all, we'll use the sentence embedding model ('all-MiniLM-L6-v2)\n",
    "\n",
    "# import\n",
    "from nlp_pipelines.vectorizer import SentenceEmbedding\n",
    "\n",
    "# initialize\n",
    "embedding_model = SentenceEmbedding(model_name='all-MiniLM-L6-v2') # \"all-MiniLM-L6-v2\" is the default for this method, but let's make it explicit\n",
    "# this is a pretrained embedding model which uses context to take a chunk of text and return a vector which is meant to represent the text in some high dimensional space. See (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) for details of this particular model; the website should also have other options for sentence embeddings, which may be more well suited to particular tasks, possibly with computational tradeoffs.\n",
    "\n",
    "# while sentence embeddings typically do well with the full sentence uncleaned, we'll also remove uninformative words and stem, to make the sentences simpler.\n",
    "\n",
    "# import\n",
    "from nlp_pipelines.preprocess import StopwordRemove, Stem\n",
    "\n",
    "# initalize\n",
    "stopword_remover = StopwordRemove() # remove uninformative words\n",
    "stemmer = Stem() # find the roots of words\n",
    "\n",
    "# let's use k-means with k=2 for the clustering\n",
    "\n",
    "#import\n",
    "from nlp_pipelines.clusterer import Kmeans\n",
    "\n",
    "# initalize\n",
    "kmeans_model = Kmeans(num_clusters=2, random_state=101) # picks 2 points in embedding space as centroids; documents are assigned to closest centroid\n",
    "\n",
    "# ok, now we need to run these on the dataset in order. Preprocess, then vectorize, then cluster.\n",
    "\n",
    "# each of the methods needs to be fit and transoformed/predicted on the data.\n",
    "print(\"initial data:\", cluster_dataset)\n",
    "stopword_remover.fit(cluster_dataset)\n",
    "cluster_dataset = stopword_remover.transform(cluster_dataset) # keep working on this object \"cluster_dataset\" step by step\n",
    "print(\"after stopword remove:\", cluster_dataset)\n",
    "stemmer.fit(cluster_dataset)\n",
    "cluster_dataset = stemmer.transform(cluster_dataset)\n",
    "print(\"after stemming:\", cluster_dataset)\n",
    "embedding_model.fit(cluster_dataset)\n",
    "cluster_dataset = embedding_model.transform(cluster_dataset)\n",
    "print(\"after vectorization:\", cluster_dataset)\n",
    "kmeans_model.fit(cluster_dataset)\n",
    "cluster_dataset = kmeans_model.predict(cluster_dataset)\n",
    "print(\"after k-means:\", cluster_dataset)\n",
    "\n",
    "# finally, let's just look at the results and original texts side by side\n",
    "print(\"original text:\", cluster_dataset.original_texts)\n",
    "print(\"cluster results:\", cluster_dataset.results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b388e",
   "metadata": {},
   "source": [
    "### Extractive Labeling (Keyword Extraction)\n",
    "\n",
    "Labeling is getting 0+ \"labels\" per document. the simpler version of this is extractive labeling, more commonly referred to as keyword extraction. The problem is usually framed as having a large text and wanting to get especially important/informative keywords. These are not picked from a user-supplied set, and are extracted from the text. Since the text is extracted, and there are multiple labels per document, I've called this extractive labeling for internal naming consistency.\n",
    "\n",
    "Anyway, we can do the same preprocessing as we did, but we don't need to vectorize, since this method works on the text itself, rather than in an embedding space. We /could/ reuse the same dataset which already has these transformations and overwrite the results, but for now, let's do it from scratch for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd299d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial data: <Dataset with 7 texts\n",
      "Texts: ['I love hiking in the mountains.', 'The sun is shining bright today.']... +5 more>\n",
      "after stopword remove: <Dataset with 7 texts\n",
      "Texts: ['love hiking mountains', 'sun shining bright today']... +5 more>\n",
      "after stemming: <Dataset with 7 texts\n",
      "Texts: ['love hike mountain', 'sun shine bright today']... +5 more>\n",
      "after labeling: <Dataset with 7 texts, results: 7 items\n",
      "Texts: ['love hike mountain', 'sun shine bright today']... +5 more\n",
      "Results: [['love', 'mountain'], ['bright today', 'sun shine']]... +5 more>\n",
      "original text: ['I love hiking in the mountains.', 'The sun is shining bright today.', 'Reading books is my favorite hobby.', 'I enjoy outdoor activities like camping.', \"It's a beautiful day for a walk in the park.\", 'I prefer staying indoors and watching movies.', 'Cooking new recipes is always fun.']\n",
      "labeling results: [['love', 'mountain'], ['bright today', 'sun shine'], ['read book', 'book'], ['activ', 'camp'], ['day walk park', 'walk park'], ['watch movi', 'stay indoor'], ['cook new recip', 'new']]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing methods were already imported above, let's re-initialize them for clarity (agin we don't /need/ to do this, we could simply fit to overwrite any information the methods got during preprocessing [in this case there's none]).\n",
    "\n",
    "stopword_remover = StopwordRemove() # remove uninformative words\n",
    "stemmer = Stem() # find the roots of words\n",
    "\n",
    "# TfidfTopN is a method which uses Term Frequency Inverse Document (corpus) frequency to find informative words, then just picks the top N on that score.\n",
    "# more or less, think of this mathod as returning words which are rare across the corpus but generally but show up in this document a lot.\n",
    "\n",
    "from nlp_pipelines.labeler import TfidfTopN\n",
    "\n",
    "# initalize\n",
    "tfidf_labeler = TfidfTopN(top_k=2, ngram_range=(1,3)) # 2 keywords per thing, up to 3gram keywords\n",
    "\n",
    "# a fresh copy of the source dataset again\n",
    "extractive_dataset = copy.deepcopy(unlabeled_dataset)\n",
    "\n",
    "# fit and transform/predict each method\n",
    "\n",
    "print(\"initial data:\", extractive_dataset)\n",
    "stopword_remover.fit(extractive_dataset)\n",
    "extractive_dataset = stopword_remover.transform(extractive_dataset) # keep working on this object \"cluster_dataset\" step by step\n",
    "print(\"after stopword remove:\", extractive_dataset)\n",
    "stemmer.fit(extractive_dataset)\n",
    "extractive_dataset = stemmer.transform(extractive_dataset)\n",
    "print(\"after stemming:\", extractive_dataset)\n",
    "tfidf_labeler.fit(extractive_dataset)\n",
    "extractive_dataset = tfidf_labeler.predict(extractive_dataset)\n",
    "print(\"after labeling:\", extractive_dataset)\n",
    "\n",
    "# finally, let's just look at the results and original texts side by side\n",
    "print(\"original text:\", extractive_dataset.original_texts)\n",
    "print(\"labeling results:\", extractive_dataset.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0496f4c",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "The pattern for classification is straightforward: one document gets one class. It's much like clustering, but we expect it to match the meaning of the class, rather than just membership. \n",
    "\n",
    "This time, however, let's use a supervised method. This also means we should split the dataset into test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "977c4034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: <Dataset with 3 texts\n",
      "Texts: ['This is terrible', 'My favorite film ever']... +1 more\n",
      "Truths: ['negative', 'positive']... +1 more>\n",
      "test split: <Dataset with 4 texts\n",
      "Texts: ['I love this movie', 'Fantastic work']... +2 more\n",
      "Truths: ['positive', 'positive']... +2 more>\n",
      "original text for test split: ['I love this movie', 'Fantastic work', 'Awful experience', 'A waste of my time']\n",
      "results from label propagation:  ['negative' 'negative' 'negative' 'negative']\n",
      "comparison: set truths ['positive', 'positive', 'negative', 'negative']\n",
      "Result evaluation {'accuracy': 0.5, 'precision_macro': 0.25, 'recall_macro': 0.5, 'f1_macro': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "train, test = classed_dataset.split(count=3, seed=101) # 3 in train, the other four in test\n",
    "\n",
    "print(\"train split:\", train)\n",
    "print(\"test split:\", test)\n",
    "\n",
    "# we've already seen preprocessing, but once more:\n",
    "stopword_remover = StopwordRemove() # remove uninformative words\n",
    "stemmer = Stem() # find the roots of words\n",
    "\n",
    "# let's use a bag of words embedding this time. This is not pretrained, so fit will actually learn something about the data. This does, however, mean that words/tokens in the test/predict data which were not in the train data will essentially be ignored under embedding. In real applciations, use a well-rounded train set.\n",
    "\n",
    "from nlp_pipelines.vectorizer import BagOfWords\n",
    "\n",
    "bag_of_words_embed = BagOfWords() # no arguments, just counts! Bag of words assigns each word/token an index for all vectors, and its value is the number of times it appears.\n",
    "\n",
    "# let's use label propogation. This is usually referred to as semi-supervised, but we can fit it on the train data then use the space of those labeled documents in the vector space to label the new documents.\n",
    "\n",
    "from nlp_pipelines.classifier import LabelProp\n",
    "\n",
    "label_prop_model = LabelProp(n_neighbors=2, kernel=\"rbf\") #by default, the kernel is knn which is probably good enough, but we're customizing here to show how to do so\n",
    "\n",
    "# ok, let's fit the train data.\n",
    "stopword_remover.fit(train)\n",
    "train = stopword_remover.transform(train)\n",
    "stemmer.fit(train)\n",
    "train = stemmer.transform(train)\n",
    "bag_of_words_embed.fit(train)\n",
    "train = bag_of_words_embed.transform(train)\n",
    "train = stemmer.transform(train)\n",
    "label_prop_model.fit(train)\n",
    "# no point in transforming, each would map to itself.\n",
    "\n",
    "# now we have each of these fit, we run it again for the test data\n",
    "test = stopword_remover.transform(test)\n",
    "test = stemmer.transform(test)\n",
    "test = bag_of_words_embed.transform(test)\n",
    "test = label_prop_model.predict(test)\n",
    "\n",
    "# look at the results\n",
    "print(\"original text for test split:\", test.original_texts)\n",
    "print(\"results from label propagation: \", test.results)\n",
    "print(\"comparison: set truths\", test.truths)\n",
    "\n",
    "# also, we can evaluate, since our results and truths are speaking the same language \n",
    "\n",
    "from nlp_pipelines.evaluate import evaluate\n",
    "\n",
    "print(\"Result evaluation\", evaluate(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ced61",
   "metadata": {},
   "source": [
    "## Usage of the pipeline constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113e00f",
   "metadata": {},
   "source": [
    "In the last part, we had a series of operations which needed to be fit then transformed. That's workable when we have all of the data at once, but we'd probably like to be able to do more later. We should save our method! However, like this this means that we'd have to save every step, and when we wanted to use it again, we'd need to use them in the same order. Remember that our methods which rely on a particular embedding space would likely break under another.\n",
    "\n",
    "To solve this, and also to make using this easier, a helper class \"Pipeline\" is used. This makes saving/loading pretrained pipelines much more workable. Also, I think it's easier to read and does not distract from the core of what the pipeline is doing.\n",
    "\n",
    "For a quick demo, let's remake the previous pipelines using Pipeline objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26cacf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text: ['I love hiking in the mountains.', 'The sun is shining bright today.', 'Reading books is my favorite hobby.', 'I enjoy outdoor activities like camping.', \"It's a beautiful day for a walk in the park.\", 'I prefer staying indoors and watching movies.', 'Cooking new recipes is always fun.']\n",
      "preprocessed text: ['love hike mountain', 'sun shine bright today', 'read book favorit hobbi', 'enjoy outdoor activ like camp', 'beauti day walk park', 'prefer stay indoor watch movi', 'cook new recip fun']\n",
      "resulting clusters: [1 1 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from nlp_pipelines.pipeline import Pipeline\n",
    "\n",
    "# clustering from previous subsecton reproduced!\n",
    "\n",
    "cluster_pipeline = Pipeline([\n",
    "    {\"name\": \"preprocess_1\", \"method\": \"preprocess.StopwordRemove\"}, # each line has the method,  the name field is just for readability\n",
    "    {\"name\": \"preprocess_2\", \"method\": \"preprocess.Stem\"}, # the method names are very similar to how the methods were imported; from nlp_pipelines.X import Y becomes X.Y for X and Y. The methods are listed in README.md\n",
    "    {\"name\": \"vectorize\", \"method\":\"vectorizer.SentenceEmbedding\", \"params\":{\"model_name\":'all-MiniLM-L6-v2'}}, # the parameters passed when initializing the method are passed as \"params\" if needed\n",
    "    {\"name\": \"kmeans\", \"method\": \"clusterer.Kmeans\", \"params\":{\"num_clusters\":2, \"random_state\":101}}\n",
    "])\n",
    "\n",
    "cluster_dataset = copy.deepcopy(unlabeled_dataset)\n",
    "\n",
    "cluster_pipeline.set_data(train_data=cluster_dataset, run_data=cluster_dataset) # same train and run data for now\n",
    "\n",
    "# run it! This is an alias for .fit and .predict\n",
    "cluster_pipeline.run()\n",
    "# get the results from the run_data\n",
    "print(\"original text:\", cluster_pipeline.run_data.original_texts)\n",
    "print(\"preprocessed text:\", cluster_pipeline.run_data.texts)\n",
    "print(\"resulting clusters:\", cluster_pipeline.run_data.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ccf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text: ['I love hiking in the mountains.', 'The sun is shining bright today.', 'Reading books is my favorite hobby.', 'I enjoy outdoor activities like camping.', \"It's a beautiful day for a walk in the park.\", 'I prefer staying indoors and watching movies.', 'Cooking new recipes is always fun.']\n",
      "preprocessed text: ['love hike mountain', 'sun shine bright today', 'read book favorit hobbi', 'enjoy outdoor activ like camp', 'beauti day walk park', 'prefer stay indoor watch movi', 'cook new recip fun']\n",
      "resulting clusters: [['love', 'day walk'], ['today', 'sun'], ['watching movies', 'day walk'], ['like', 'outdoor'], ['day walk', 'walk park'], ['prefer', 'watching movies'], ['new', 'fun']]\n"
     ]
    }
   ],
   "source": [
    "# extractive labeling from previous subsecton reproduced!\n",
    "\n",
    "extract_pipeline = Pipeline([\n",
    "    {\"name\": \"preprocess_1\", \"method\": \"preprocess.StopwordRemove\"},\n",
    "    {\"name\": \"preprocess_2\", \"method\": \"preprocess.Stem\"},\n",
    "    {\"name\": \"tfidf keyword extract\", \"method\": \"labeler.TfidfTopN\", \"params\":{\"top_k\":2, \"ngram_range\":(1,3)}}\n",
    "])\n",
    "\n",
    "extractive_dataset = copy.deepcopy(unlabeled_dataset)\n",
    "\n",
    "extract_pipeline.set_data(train_data=extractive_dataset, run_data=extractive_dataset) # same train and run data for now, again since no split\n",
    "\n",
    "# run it! This is an alias for .fit and .predict\n",
    "extract_pipeline.run()\n",
    "# get the results from the run_data\n",
    "print(\"original text:\", extract_pipeline.run_data.original_texts)\n",
    "print(\"preprocessed text:\", extract_pipeline.run_data.texts)\n",
    "print(\"resulting keyword sets:\", extract_pipeline.run_data.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55810042",
   "metadata": {},
   "source": [
    "## Saving and loading trained pipeline\n",
    "\n",
    "For the classifier, we had to fit a lot! It's a good case to save and load a pipeline: there's seprate data, and both the vectorizer and the model have changed from .fit. Let's do the same thing this time though let's pretend we're picking it up after some time and save/load the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672cad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text: ['I love this movie', 'Fantastic work', 'Awful experience', 'A waste of my time']\n",
      "preprocessed text: ['love movi', 'fantast work', 'aw experi', 'wast time']\n",
      "resulting classes: ['negative' 'negative' 'negative' 'negative']\n",
      "true classes: ['positive', 'positive', 'negative', 'negative']\n",
      "evaluation of classes: {'accuracy': 0.5, 'precision_macro': 0.25, 'recall_macro': 0.5, 'f1_macro': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "# classification from previous subsecton reproduced!\n",
    "\n",
    "extract_pipeline = Pipeline([\n",
    "    {\"name\": \"preprocess_1\", \"method\": \"preprocess.StopwordRemove\"},\n",
    "    {\"name\": \"preprocess_2\", \"method\": \"preprocess.Stem\"},\n",
    "    {\"name\": \"bow vectorize\", \"method\": \"vectorizer.BagOfWords\"},\n",
    "    {\"name\": \"classifier labelprop\", \"method\": \"classifier.LabelProp\", \"params\":{\"n_neighbors\":2, \"kernel\":\"rbf\"}}\n",
    "])\n",
    "\n",
    "train, test = classed_dataset.split(count=3, seed=101)\n",
    "\n",
    "extract_pipeline.set_data(train_data=train) # just need train data to train\n",
    "extract_pipeline.train()\n",
    "\n",
    "extract_pipeline.save(\"./demo_data/toy_classifier.pickle\")\n",
    "\n",
    "# let's load it\n",
    "loaded_pipeline = Pipeline.load(\"./demo_data/toy_classifier.pickle\")\n",
    "# now let's load the test data and run the saved pretrained pipeline on that\n",
    "loaded_pipeline.set_data(run_data=test)\n",
    "loaded_pipeline.predict()\n",
    "\n",
    "# get the results from the run_data\n",
    "print(\"original text:\", loaded_pipeline.run_data.original_texts)\n",
    "print(\"preprocessed text:\", loaded_pipeline.run_data.texts)\n",
    "print(\"resulting classes:\", loaded_pipeline.run_data.results)\n",
    "print(\"true classes:\", loaded_pipeline.run_data.truths)\n",
    "\n",
    "# and again, evaluate\n",
    "print(\"evaluation of classes:\", evaluate(loaded_pipeline.run_data))\n",
    "\n",
    "# see, just the same! but easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4fef7",
   "metadata": {},
   "source": [
    "# Practical Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fdacbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset, what it is and why it's used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c09249",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885856c8",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925cd745",
   "metadata": {},
   "source": [
    "## Extractive Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc54c0c8",
   "metadata": {},
   "source": [
    "## Predictive Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200e57a",
   "metadata": {},
   "source": [
    "## Mixing Methods\n",
    "WIP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
