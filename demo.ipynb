{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3639e010",
   "metadata": {},
   "source": [
    "# NLP Pipelines\n",
    "\n",
    "This package is designed to generalize the fit/predict to entire end to end pipelines for text. It can be used as individual methods or as a pipeline object to manage execution of training and/or testing. See README.md for an overview of which methods are in here.\n",
    "\n",
    "## This Demo Set\n",
    "\n",
    "This set of demos walks through:\n",
    "\n",
    "* **Basic Usage Demos** Using abstract/toy datasets to show how to use this library.\n",
    "  * **Usage of the library as individual methods**: Using each of the methods manually to get results.\n",
    "  * **Usage of the pipeline constructor**: A reimplementaiton of the examples above using the pipeline object.\n",
    "  * **Saving and loading trained pipelines**: Quick guidance on how to separate training and running/prediction.\n",
    "* **Practical Demos**: With real data and justifying decisions on pipeline design.\n",
    "  * **Practical Demo: Clustering**: A real end-to-end example on clustering, where \"clustering\" is picking some set of groups without explicit mapping to classes.\n",
    "  * **Practical Demo: Classification**: A real end-to-end example on classification, where \"classificaiton\" is picking which of a set of classes best apply to a document.\n",
    "  * **Practical Demo: Extractive Labeling**: A real end-to-end example on extractive labeling, where \"extractive labeling\" is picking which words or phrases in a document \"seem important\", like automatic keyword extraction.\n",
    "  * **Practical Demo: Extractive Labeling**: A real end-to-end example on predictive labeling, where a set of possible labels is chosen a prioro, and between none and all of the possible labels may apply to a document.\n",
    "  * **(WIP) Practical Demo: Mixing Methods**:  A real end-to-end example on using different kinds of methods together to get simpler results.\n",
    "\n",
    "NOTE that this is not intended to be an exhaustive demo of all of the different methods: `test_demos.ipynb` may be a better choice for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ad3c6",
   "metadata": {},
   "source": [
    "## Setup Dependncies\n",
    "For portability, this is included to install dependencies for this library and notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc01a498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (3.9.1)\n",
      "Requirement already satisfied: spacy in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (3.8.7)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.2.6)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (2.3.1)\n",
      "Requirement already satisfied: fasttext in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.9.3)\n",
      "Requirement already satisfied: sentence_transformers in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (5.0.0)\n",
      "Requirement already satisfied: rank_bm25 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (0.2.2)\n",
      "Requirement already satisfied: keybert in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.9.0)\n",
      "Requirement already satisfied: multi_rake in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.0.2)\n",
      "Requirement already satisfied: yake in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (0.6.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (2.7.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (3.5)\n",
      "Requirement already satisfied: xgboost in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (3.0.2)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (4.54.1)\n",
      "Requirement already satisfied: hdbscan in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (0.8.40)\n",
      "Requirement already satisfied: umap in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (0.1.1)\n",
      "Requirement already satisfied: pyarrow in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (21.0.0)\n",
      "Requirement already satisfied: bertopic in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (0.17.3)\n",
      "Requirement already satisfied: umap-learn in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (0.5.9.post2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 2)) (2025.7.29)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 5)) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from fasttext->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 7)) (0.34.3)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 7)) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 7)) (4.14.1)\n",
      "Requirement already satisfied: rich>=10.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from keybert->-r requirements.txt (line 11)) (14.0.0)\n",
      "Requirement already satisfied: pycld2>=0.41 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from multi_rake->-r requirements.txt (line 12)) (0.42)\n",
      "Requirement already satisfied: pyrsistent>=0.14.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from multi_rake->-r requirements.txt (line 12)) (0.20.0)\n",
      "Requirement already satisfied: jellyfish in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from yake->-r requirements.txt (line 13)) (1.2.0)\n",
      "Requirement already satisfied: segtok in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from yake->-r requirements.txt (line 13)) (1.5.11)\n",
      "Requirement already satisfied: tabulate in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from yake->-r requirements.txt (line 13)) (0.9.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from torch->-r requirements.txt (line 14)) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from torch->-r requirements.txt (line 14)) (1.14.0)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from torch->-r requirements.txt (line 14)) (2025.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (0.5.3)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from bertopic->-r requirements.txt (line 21)) (6.2.0)\n",
      "Requirement already satisfied: llvmlite>0.36.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from bertopic->-r requirements.txt (line 21)) (0.44.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from umap-learn->-r requirements.txt (line 22)) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from umap-learn->-r requirements.txt (line 22)) (0.5.13)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers->-r requirements.txt (line 7)) (1.1.5)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from plotly>=4.7.0->bertopic->-r requirements.txt (line 21)) (2.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (2025.7.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from rich>=10.4.0->keybert->-r requirements.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from rich>=10.4.0->keybert->-r requirements.txt (line 11)) (2.19.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 14)) (1.3.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 3)) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 3)) (1.5.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 3)) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 3)) (7.3.0.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from jinja2->spacy->-r requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert->-r requirements.txt (line 11)) (0.1.2)\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 3)) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "%pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6366d56",
   "metadata": {},
   "source": [
    "# Basic Usage Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71427222",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We should make different datasets for different pipelines. For this basic usage section, the datasets are fake and chosen unrealistically to demonstrate the shape of the inputs and outputs of the steps and pipelines. We'll need a dataset with no truths (true labels are called \"truths\" in this library) for clustering and extractive labeling (n.b. this is not strictly necessary, clustering and extractive labeling methods will simply ignore truths, so including them here is likely to be confusing), one with classes for classification, and one with lists of labels as truths for labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e188325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_pipelines.dataset import Dataset\n",
    "\n",
    "# methods which do not need truths of any form get the unlabeled_dataset.\n",
    "texts_1 = [\n",
    "    \"I love hiking in the mountains.\",\n",
    "    \"The sun is shining bright today.\",\n",
    "    \"Reading books is my favorite hobby.\",\n",
    "    \"I enjoy outdoor activities like camping.\",\n",
    "    \"It's a beautiful day for a walk in the park.\",\n",
    "    \"I prefer staying indoors and watching movies.\",\n",
    "    \"Cooking new recipes is always fun.\"\n",
    "]\n",
    "\n",
    "unlabeled_dataset = Dataset(texts_1)\n",
    "\n",
    "# methods which need truth clases (exactly one per document) get the classed_dataset.\n",
    "texts_2 = [\n",
    "    \"I love this movie\",\n",
    "    \"This is terrible\",\n",
    "    \"Fantastic work\",\n",
    "    \"Awful experience\",\n",
    "    \"My favorite film ever\",\n",
    "    \"A waste of my time\",\n",
    "    \"Not enough people have seen this masterpiece\"\n",
    "    ]\n",
    "classes = [\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"positive\"\n",
    "    ]\n",
    "classed_dataset = Dataset(texts_2, classes)\n",
    "\n",
    "# methods which need truth label sets (0 to n per document) get the labeled_dataset.\n",
    "texts_3 = [\n",
    "    \"I love baking with things from my garden.\",\n",
    "    \"Here's a recipe for lasagna.\",\n",
    "    \"I enjoy hiking in the mountains and camping.\",\n",
    "    \"I have recenly learned how to cook good food when hiking.\",\n",
    "    \"I spent the weekend building a treehouse.\",\n",
    "    \"Cooking dinner with fresh ingredients is a joy.\",\n",
    "    \"I often take long walks in the park during summer to get inspired on what I should plant.\"\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    [\"cooking\", \"gardening\"],\n",
    "    [\"cooking\"],\n",
    "    [\"outdoors\"],\n",
    "    [\"outdoors\", \"cooking\"],\n",
    "    [\"outdoors\"],\n",
    "    [\"cooking\"],\n",
    "    [\"outdoors\", \"gardening\"]\n",
    "]\n",
    "\n",
    "labeled_dataset = Dataset(texts_3, classes)\n",
    "\n",
    "#also, for convenience, let's add an explicit list of possible_labels\n",
    "possible_labels = [\"gardening\", \"cooking\", \"outdoors\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f8f8c",
   "metadata": {},
   "source": [
    "## Usage of the library as individual methods\n",
    "\n",
    "Now, let's try to get some results for our toy data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8311467",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "As a reminder, we want to apply clusters on the unlabeled dataset.\n",
    "For now, we'll judge the results on what *feels* right for such a small dataset with no truth for comparison; proper evaluation will be explored in the next section on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dc54522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial data: <Dataset with 7 texts\n",
      "Texts: ['I love hiking in the mountains.', 'The sun is shining bright today.']... +5 more>\n",
      "after stopword remove: <Dataset with 7 texts\n",
      "Texts: ['love hiking mountains', 'sun shining bright today']... +5 more>\n",
      "after stemming: <Dataset with 7 texts\n",
      "Texts: ['love hike mountain', 'sun shine bright today']... +5 more>\n",
      "after vectorization: <Dataset with 7 texts, vectors: 384-dim\n",
      "Texts: ['love hike mountain', 'sun shine bright today']... +5 more>\n",
      "after k-means: <Dataset with 7 texts, vectors: 384-dim, results: 7 items\n",
      "Texts: ['love hike mountain', 'sun shine bright today']... +5 more\\Results: [1 1]... +5 more>\n",
      "original text: ['I love hiking in the mountains.', 'The sun is shining bright today.', 'Reading books is my favorite hobby.', 'I enjoy outdoor activities like camping.', \"It's a beautiful day for a walk in the park.\", 'I prefer staying indoors and watching movies.', 'Cooking new recipes is always fun.']\n",
      "cluster results: [1 1 1 0 1 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# let's make a copy of our data as not to interfere with any other runs\n",
    "import copy\n",
    "\n",
    "cluster_dataset = copy.deepcopy(unlabeled_dataset)\n",
    "\n",
    "#Our goal is to see which clusters the data is in, rather than to use the clusters to classify new data, so we can use the same data to fit and predict without splitting, just using cluster_dataset\n",
    "\n",
    "# we need a vectorizer to put this into a form which we can do math on. So we can use one thing for all, we'll use the sentence embedding model ('all-MiniLM-L6-v2)\n",
    "\n",
    "# import\n",
    "from nlp_pipelines.vectorizer import SentenceEmbedding\n",
    "\n",
    "# initialize\n",
    "embedding_model = SentenceEmbedding(model_name='all-MiniLM-L6-v2') # \"all-MiniLM-L6-v2\" is the default for this method, but let's make it explicit\n",
    "# this is a pretrained embedding model which uses context to take a chunk of text and return a vector which is meant to represent the text in some high dimensional space. See (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) for details of this particular model; the website should also have other options for sentence embeddings, which may be more well suited to particular tasks, possibly with computational tradeoffs.\n",
    "\n",
    "# while sentence embeddings typically do well with the full sentence uncleaned, we'll also remove uninformative words and stem, to make the sentences simpler.\n",
    "\n",
    "# import\n",
    "from nlp_pipelines.preprocess import StopwordRemove, Stem\n",
    "\n",
    "# initalize\n",
    "stopword_remover = StopwordRemove() # remove uninformative words\n",
    "stemmer = Stem() # find the roots of words\n",
    "\n",
    "# let's use k-means with k=2 for the clustering\n",
    "\n",
    "#import\n",
    "from nlp_pipelines.clusterer import Kmeans\n",
    "\n",
    "# initalize\n",
    "kmeans_model = Kmeans(num_clusters=2, random_state=101) # picks 2 points in embedding space as centroids; documents are assigned to closest centroid\n",
    "\n",
    "# ok, now we need to run these on the dataset in order. Preprocess, then vectorize, then cluster.\n",
    "\n",
    "# each of the methods needs to be fit and transoformed/predicted on the data.\n",
    "print(\"initial data:\", cluster_dataset)\n",
    "stopword_remover.fit(cluster_dataset)\n",
    "cluster_dataset = stopword_remover.transform(cluster_dataset) # keep working on this object \"cluster_dataset\" step by step\n",
    "print(\"after stopword remove:\", cluster_dataset)\n",
    "stemmer.fit(cluster_dataset)\n",
    "cluster_dataset = stemmer.transform(cluster_dataset)\n",
    "print(\"after stemming:\", cluster_dataset)\n",
    "embedding_model.fit(cluster_dataset)\n",
    "cluster_dataset = embedding_model.transform(cluster_dataset)\n",
    "print(\"after vectorization:\", cluster_dataset)\n",
    "kmeans_model.fit(cluster_dataset)\n",
    "cluster_dataset = kmeans_model.predict(cluster_dataset)\n",
    "print(\"after k-means:\", cluster_dataset)\n",
    "\n",
    "# finally, let's just look at the results and original texts side by side\n",
    "print(\"original text:\", cluster_dataset.original_texts)\n",
    "print(\"cluster results:\", cluster_dataset.results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b388e",
   "metadata": {},
   "source": [
    "### Extractive Labeling (Keyword Extraction)\n",
    "\n",
    "Labeling is getting 0+ \"labels\" per document. the simpler version of this is extractive labeling, more commonly referred to as keyword extraction. The problem is usually framed as having a large text and wanting to get especially important/informative keywords. These are not picked from a user-supplied set, and are extracted from the text. Since the text is extracted, and there are multiple labels per document, I've called this extractive labeling for internal naming consistency.\n",
    "\n",
    "Anyway, we can do the same preprocessing as we did, but we don't need to vectorize, since this method works on the text itself, rather than in an embedding space. We /could/ reuse the same dataset which already has these transformations and overwrite the results, but for now, let's do it from scratch for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd299d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial data: <Dataset with 7 texts\n",
      "Texts: ['I love hiking in the mountains.', 'The sun is shining bright today.']... +5 more>\n",
      "after stopword remove: <Dataset with 7 texts\n",
      "Texts: ['love hiking mountains', 'sun shining bright today']... +5 more>\n",
      "after stemming: <Dataset with 7 texts\n",
      "Texts: ['love hike mountain', 'sun shine bright today']... +5 more>\n",
      "after labeling: <Dataset with 7 texts, results: 7 items\n",
      "Texts: ['love hike mountain', 'sun shine bright today']... +5 more\\Results: [['love', 'hike'], ['today', 'sun']]... +5 more>\n",
      "original text: ['I love hiking in the mountains.', 'The sun is shining bright today.', 'Reading books is my favorite hobby.', 'I enjoy outdoor activities like camping.', \"It's a beautiful day for a walk in the park.\", 'I prefer staying indoors and watching movies.', 'Cooking new recipes is always fun.']\n",
      "labeling results: [['love', 'hike'], ['today', 'sun'], ['book', 'read'], ['activ', 'like'], ['beauti', 'day'], ['watch', 'indoor'], ['recip', 'cook']]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing methods were already imported above, let's re-initialize them for clarity (agin we don't /need/ to do this, we could simply fit to overwrite any information the methods got during preprocessing [in this case there's none]).\n",
    "\n",
    "stopword_remover = StopwordRemove() # remove uninformative words\n",
    "stemmer = Stem() # find the roots of words\n",
    "\n",
    "# TfidfTopN is a method which uses Term Frequency Inverse Document (corpus) frequency to find informative words, then just picks the top N on that score.\n",
    "# more or less, think of this mathod as returning words which are rare across the corpus but generally but show up in this document a lot.\n",
    "\n",
    "from nlp_pipelines.labeler import TfidfTopN\n",
    "\n",
    "# initalize\n",
    "tfidf_labeler = TfidfTopN(top_k=2, ngram_range=(1,3)) # 2 keywords per thing, up to 3gram keywords\n",
    "\n",
    "# a fresh copy of the source dataset again\n",
    "extractive_dataset = copy.deepcopy(unlabeled_dataset)\n",
    "\n",
    "# fit and transform/predict each method\n",
    "\n",
    "print(\"initial data:\", extractive_dataset)\n",
    "stopword_remover.fit(extractive_dataset)\n",
    "extractive_dataset = stopword_remover.transform(extractive_dataset) # keep working on this object \"cluster_dataset\" step by step\n",
    "print(\"after stopword remove:\", extractive_dataset)\n",
    "stemmer.fit(extractive_dataset)\n",
    "extractive_dataset = stemmer.transform(extractive_dataset)\n",
    "print(\"after stemming:\", extractive_dataset)\n",
    "tfidf_labeler.fit(extractive_dataset)\n",
    "extractive_dataset = tfidf_labeler.predict(extractive_dataset)\n",
    "print(\"after labeling:\", extractive_dataset)\n",
    "\n",
    "# finally, let's just look at the results and original texts side by side\n",
    "print(\"original text:\", extractive_dataset.original_texts)\n",
    "print(\"labeling results:\", extractive_dataset.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ced61",
   "metadata": {},
   "source": [
    "## Usage of the pipeline constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db5b1f",
   "metadata": {},
   "source": [
    "## Saving and loading trained pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4fef7",
   "metadata": {},
   "source": [
    "# Practical Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fdacbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset, what it is and why it's used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c09249",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885856c8",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925cd745",
   "metadata": {},
   "source": [
    "## Extractive Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc54c0c8",
   "metadata": {},
   "source": [
    "## Predictive Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200e57a",
   "metadata": {},
   "source": [
    "## Mixing Methods\n",
    "WIP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
