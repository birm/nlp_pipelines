{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d5d388",
   "metadata": {},
   "source": [
    "# NLP Pipelines Demos\n",
    "\n",
    "This notebook shows a few demos to help understand the nlp pipelines package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446c4d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (3.9.1)\n",
      "Requirement already satisfied: spacy in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (3.8.7)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.3.2)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (2.3.1)\n",
      "Requirement already satisfied: fasttext in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.9.3)\n",
      "Requirement already satisfied: sentence_transformers in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (5.0.0)\n",
      "Requirement already satisfied: rank_bm25 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (0.2.2)\n",
      "Requirement already satisfied: keybert in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.9.0)\n",
      "Requirement already satisfied: multi_rake in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.0.2)\n",
      "Requirement already satisfied: yake in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (0.6.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (2.7.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (3.5)\n",
      "Requirement already satisfied: xgboost in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (3.0.2)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (4.54.1)\n",
      "Requirement already satisfied: hdbscan in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (0.8.40)\n",
      "Requirement already satisfied: umap in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (0.1.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 2)) (2025.7.29)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 5)) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from fasttext->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 7)) (0.34.3)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 7)) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 7)) (4.14.1)\n",
      "Requirement already satisfied: rich>=10.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from keybert->-r requirements.txt (line 11)) (14.0.0)\n",
      "Requirement already satisfied: pycld2>=0.41 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from multi_rake->-r requirements.txt (line 12)) (0.42)\n",
      "Requirement already satisfied: pyrsistent>=0.14.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from multi_rake->-r requirements.txt (line 12)) (0.20.0)\n",
      "Requirement already satisfied: jellyfish in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from yake->-r requirements.txt (line 13)) (1.2.0)\n",
      "Requirement already satisfied: segtok in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from yake->-r requirements.txt (line 13)) (1.5.11)\n",
      "Requirement already satisfied: tabulate in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from yake->-r requirements.txt (line 13)) (0.9.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from torch->-r requirements.txt (line 14)) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from torch->-r requirements.txt (line 14)) (1.14.0)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from torch->-r requirements.txt (line 14)) (2025.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 17)) (0.5.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers->-r requirements.txt (line 7)) (1.1.5)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 3)) (2025.7.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from rich>=10.4.0->keybert->-r requirements.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from rich>=10.4.0->keybert->-r requirements.txt (line 11)) (2.19.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 14)) (1.3.0)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 3)) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 3)) (1.5.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 3)) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 3)) (7.3.0.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from jinja2->spacy->-r requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert->-r requirements.txt (line 11)) (0.1.2)\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 3)) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "%pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d535248e",
   "metadata": {},
   "source": [
    "## Example: Simple Data, Clustered\n",
    "\n",
    "For perhaps the simplest example, let's take a small toy dataset/corpus and make clusters. First we'll use methods individually, then we'll use the pipeline object to help simplify.\n",
    "\n",
    "### Using Specific Methods\n",
    "\n",
    "You can use any of the methods directly. For example, we can clean text or use a bag of words vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4480a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset with 8 texts\n",
      "Texts: ['The new stethoscope model by Littmann is available now.', 'Philips unveils an innovative heart monitor with improved accuracy.']... +6 more>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nlp_pipelines.vectorizer import Bow\n",
    "from nlp_pipelines.dataset import Dataset\n",
    "from nlp_pipelines import preprocess\n",
    "\n",
    "# First, a simple dataset for demonstration\n",
    "texts = [\n",
    "    \"The new stethoscope model by Littmann is available now.\",\n",
    "    \"Philips unveils an innovative heart monitor with improved accuracy.\",\n",
    "    \"Medtronic announces a breakthrough in robotic surgery technology.\",\n",
    "    \"GE Healthcare's ultrasound device provides high-definition imaging.\",\n",
    "    \"Stryker introduces a new orthopedic surgical tool.\",\n",
    "    \"Johnson & Johnson releases a new line of surgical instruments.\",\n",
    "    \"Siemens Healthineers develops a state-of-the-art MRI scanner.\",\n",
    "    \"Boston Scientific launches a catheter designed for heart surgery.\"\n",
    "]\n",
    "dataset = Dataset(texts)\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471d67dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new stethoscope model Littmann available', 'Philips unveils innovative heart monitor improved accuracy', 'Medtronic announces breakthrough robotic surgery technology', 'GE Healthcare ultrasound device provides high definition imaging', 'Stryker introduces new orthopedic surgical tool', 'Johnson Johnson releases new line surgical instruments', 'Siemens Healthineers develops state art MRI scanner', 'Boston Scientific launches catheter designed heart surgery']\n"
     ]
    }
   ],
   "source": [
    "# let's remove stopwords (uninformative words)\n",
    "stopword_remover = preprocess.StopwordRemove()\n",
    "\n",
    "dataset = stopword_remover.transform(dataset)\n",
    "print(dataset.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dec76d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new stethoscope model Littmann available', 'philip unveil innovative heart monitor improved accuracy', 'medtronic announce breakthrough robotic surgery technology', 'GE Healthcare ultrasound device provide high definition imaging', 'stryker introduce new orthopedic surgical tool', 'Johnson Johnson release new line surgical instrument', 'Siemens Healthineers develop state art MRI scanner', 'Boston Scientific launch catheter design heart surgery']\n"
     ]
    }
   ],
   "source": [
    "# let's lemmatize to see what that does too\n",
    "\n",
    "lemmatizer = preprocess.Lemmatize()\n",
    "\n",
    "dataset = lemmatizer.transform(dataset)\n",
    "print(dataset.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "172c0ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset with 8 texts, vectors: 47-dim\n",
      "Texts: ['new stethoscope model Littmann available', 'philip unveil innovative heart monitor improved accuracy']... +6 more>\n",
      "[[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      "  0 0 0 0 1 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 1 0 0 0 0 0 1 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      "  0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# ok, maybe this is reasonable to vectorize?\n",
    "\n",
    "vectorizer = Bow()\n",
    "\n",
    "vectorizer.fit(dataset)\n",
    "dataset = vectorizer.transform(dataset) # for now, the same dataset since it's all we have\n",
    "\n",
    "print(dataset) # show us the state of the dataset\n",
    "print(dataset.vectors) # show us just the vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b64187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset with 8 texts, vectors: 47-dim, results: 8 items\n",
      "Texts: ['new stethoscope model Littmann available', 'philip unveil innovative heart monitor improved accuracy']... +6 more\\Results: [1 1]... +6 more>\n",
      "[1 1 1 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# to make this end to end, let's try to cluster to see what that gets us\n",
    "from nlp_pipelines.clusterer import Kmeans\n",
    "\n",
    "model = Kmeans(num_clusters=2, random_state=101)\n",
    "\n",
    "model.fit(dataset)\n",
    "dataset = model.predict(dataset)\n",
    "\n",
    "print(dataset) # what's in the dataset now\n",
    "print(dataset.results) # what are the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8fae77",
   "metadata": {},
   "source": [
    "### Using a pipeline\n",
    "\n",
    "Instead of doing these one by one, a helper class \"Pipeline\" lets us define these as a pipeline and run them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59fd867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [1 1 1 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from nlp_pipelines.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    {\"name\": \"preproc1\", \"method\": \"preprocess.StopwordRemove\"},\n",
    "    {\"name\": \"preproc2\", \"method\": \"preprocess.Lemmatize\"},\n",
    "    {\"name\": \"vectorize\", \"method\": \"vectorizer.Bow\"},\n",
    "    {\"name\": \"cluster\", \"method\": \"clusterer.Kmeans\", \"params\":{\"num_clusters\":2, \"random_state\": 101}}\n",
    "])\n",
    "pipeline.set_data(train_data=dataset, run_data=dataset) # for now, train and run on the same data\n",
    "pipeline.run()\n",
    "\n",
    "print(\"Results:\", pipeline.run_data.results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc37670",
   "metadata": {},
   "source": [
    "Same results, since it's the same pipeline.\n",
    "\n",
    "Also, the intermediate results are still part of the pipeline's dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5648071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original texts: ['The new stethoscope model by Littmann is available now.', 'Philips unveils an innovative heart monitor with improved accuracy.', 'Medtronic announces a breakthrough in robotic surgery technology.', \"GE Healthcare's ultrasound device provides high-definition imaging.\", 'Stryker introduces a new orthopedic surgical tool.', 'Johnson & Johnson releases a new line of surgical instruments.', 'Siemens Healthineers develops a state-of-the-art MRI scanner.', 'Boston Scientific launches a catheter designed for heart surgery.']\n",
      "Preprocessed texts: ['new stethoscope model Littmann available', 'philip unveil innovative heart monitor improved accuracy', 'medtronic announce breakthrough robotic surgery technology', 'GE Healthcare ultrasound device provide high definition imaging', 'stryker introduce new orthopedic surgical tool', 'Johnson Johnson release new line surgical instrument', 'Siemens Healthineers develop state art MRI scanner', 'Boston scientific launch catheter design heart surgery']\n",
      "Vectors: [[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      "  0 0 0 0 1 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 1 0 0 0 0 0 1 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      "  0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# the dataset keeps the last of the other things it's seen\n",
    "\n",
    "# original text\n",
    "print(\"Original texts:\", pipeline.run_data.original_texts)\n",
    "# preprocessed text\n",
    "print(\"Preprocessed texts:\", pipeline.run_data.texts)\n",
    "# vectors\n",
    "print(\"Vectors:\", pipeline.run_data.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b4caa2",
   "metadata": {},
   "source": [
    "## Example: Simple Data, Classified\n",
    "\n",
    "Now, let's pick a dataset with truths and use that to classify the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa601e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset with 5 texts\n",
      "Texts: ['I love this movie', 'This is terrible']... +3 more\n",
      "Truths: ['positive', 'negative']... +3 more>\n",
      "<Dataset with 3 texts\n",
      "Texts: ['I love this movie', 'This is terrible']... +1 more\n",
      "Truths: ['positive', 'negative']... +1 more> <Dataset with 2 texts\n",
      "Texts: ['Awful experience', 'It was okay']\n",
      "Truths: ['negative', 'neutral']>\n"
     ]
    }
   ],
   "source": [
    "texts = [\"I love this movie\", \"This is terrible\", \"Fantastic work\", \"Awful experience\", \"It was okay\"]\n",
    "truths = [\"positive\", \"negative\", \"positive\", \"negative\", \"neutral\"]\n",
    "dataset = Dataset(texts, truths)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "train, test = dataset.split(count=3)\n",
    "print(train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36f0b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/spade/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: ['positive' 'positive']\n",
      "Truths: ['negative', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "# the words are quite different, so low co-occurence is going to break tfidf/bow; let's try to embed with a sentence embedding to get context!\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    {\"name\": \"vectorize\", \"method\": \"vectorizer.SentenceEmbedding\"},\n",
    "    {\"name\": \"classify\", \"method\": \"classifier.Xgboost\"}\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.set_data(train_data=train, run_data=test) # now we have different train and test data!\n",
    "pipeline.run()\n",
    "\n",
    "print(\"Results:\", pipeline.run_data.results)\n",
    "print(\"Truths:\", pipeline.run_data.truths) # TODO evaluation code was not finished for all result types as of the writing of this.\n",
    "\n",
    "# I ran it without a seed, and got 1/2 right, so maybe three sentences isn't enough to train a tree model ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3212e047",
   "metadata": {},
   "source": [
    "## Example: Simple Data, Labeled\n",
    "\n",
    "Finally, we have labeling (formerly \"keyword extraction\"); retuning 0 to n labels.\n",
    "However, we have two different kinds of labelers: extractive ones pick important words from the document. Predictive ones take the list of labels and try to predict which apply to text.\n",
    "\n",
    "In any case, let's start with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59fda05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset with 5 texts\n",
      "Texts: ['Patient shows symptoms of fever and cough, possible pneumonia diagnosis.', 'Headache and nausea reported, likely migraine.']... +3 more\n",
      "Truths: [['pneumonia', 'respiratory infection'], ['migraine']]... +3 more>\n"
     ]
    }
   ],
   "source": [
    "# common dataset\n",
    "texts = [\n",
    "    \"Patient shows symptoms of fever and cough, possible pneumonia diagnosis.\",\n",
    "    \"Headache and nausea reported, likely migraine.\",\n",
    "    \"Frequent urination and fatigue, potential diabetes condition.\",\n",
    "    \"Coughing and shortness of breath, indicative of respiratory infection.\",\n",
    "    \"Reports of dizziness, nausea, and blurred vision, possible stroke.\"\n",
    "]\n",
    "\n",
    "truths = [\n",
    "    [\"pneumonia\", \"respiratory infection\"],\n",
    "    [\"migraine\"],\n",
    "    [\"diabetes\"],\n",
    "    [\"respiratory infection\"],\n",
    "    [\"stroke\"]\n",
    "]\n",
    "\n",
    "possible_labels = [\"pneumonia\", \"migraine\", \"diabetes\", \"respiratory infection\", \"stroke\"]\n",
    "\n",
    "# Create the Dataset object\n",
    "dataset = Dataset(texts, truths)\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441c1d8",
   "metadata": {},
   "source": [
    "### Extractive Labeling\n",
    "What are the top 2 words according to an extractive labeler? Let's clean a little bit then try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abbb3aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [['patient show symptom', 'patient show'], ['Headache and nausea', 'nausea report'], ['potential diabetes condition', 'frequent urination'], ['cough and shortness', 'shortness of breath'], ['report of dizziness', 'nausea']]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    {\"name\": \"preprocess\", \"method\": \"preprocess.Lemmatize\"},\n",
    "    {\"name\": \"extract\", \"method\": \"labeler.Yake\", \"params\":{\"top_k\":2}}\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.set_data(train_data=dataset, run_data=dataset)\n",
    "pipeline.run()\n",
    "\n",
    "print(\"Results:\", pipeline.run_data.results) # not sure how to best evaluate extractive keywords in our context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d924858",
   "metadata": {},
   "source": [
    "## Predictive labeling\n",
    "Let's use a method to predict which keywords from our list seem to best apply. We'll need to embed more things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d499ef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [['migraine', 'stroke', 'pneumonia', 'diabetes', 'respiratory infection'], ['diabetes', 'respiratory infection', 'migraine', 'stroke', 'pneumonia']]\n",
      "Truths: [['migraine'], ['diabetes']]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    {\"name\": \"vectorize\", \"method\": \"vectorizer.SentenceEmbedding\"},\n",
    "    {\"name\": \"predict\", \"method\": \"labeler.ThresholdSim\"}\n",
    "])\n",
    "\n",
    "# we overwrite the previous instance of the dataset, so let's make a clean copy\n",
    "dataset = Dataset(texts, truths)\n",
    "train, test = dataset.split(count=3)\n",
    "\n",
    "pipeline.set_data(train_data=train, run_data=test, possible_labels=possible_labels)\n",
    "pipeline.run()\n",
    "\n",
    "print(\"Results:\", pipeline.run_data.results) # TODO! it only SORTS the keywords on distance right now! Do the threshold.\n",
    "print(\"Truths:\", pipeline.run_data.truths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
